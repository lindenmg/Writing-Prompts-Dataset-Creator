{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2024 Gabriel Lindenmaier\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# In case of Jupyter notebooks leave out the __file__ variable.\n",
    "# AND ensure that the combination of \"..\" leads to the root directory\n",
    "project_root_path = os.path.realpath(os.path.join(\"../\"))\n",
    "sys.path.append(project_root_path)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "from src.preprocessing.dataset_creation import DataSetCreator, encode_ids_prompt_bert, encode_ids_sents_bpe\n",
    "from src.utils.settings import Config\n",
    "from src.data.data_exploration import DataExplorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants & Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_cores = Config.hardware.n_cpu\n",
    "explorer = DataExplorer()\n",
    "\n",
    "# ToDo: Place into sensible configuration file.\n",
    "# IMPORTANT !!!   !!!   !!!   !!!\n",
    "# Determine how long a story, measured by token count, is allowed to be at maximum\n",
    "story_token_limit = 1024\n",
    "story_word_limit = 700\n",
    "joint_vocab = True\n",
    "vocab_size = 24576\n",
    "technique = 'unigram'  # 'unigram' or 'bpe'\n",
    "vocab_name = f\"{technique}_{story_word_limit}w_{story_token_limit}t_{vocab_size // 1000}k\"\n",
    "print(vocab_name)\n",
    "\n",
    "dataset_creator = DataSetCreator(vocab_name=vocab_name\n",
    "                                 , vocab_size=vocab_size\n",
    "                                 , use_joint_vocab=joint_vocab\n",
    "                                 , tokenization=technique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base = Config.path.data_base\n",
    "sql_query = f\"\"\"\n",
    "SELECT f.prompt, f.prompt_body, f.story, f.prompt_score, f.story_score, f.story_words\n",
    "FROM filtered as f\n",
    "WHERE f.story_words >= 100 and f.story_words <= {story_word_limit}\n",
    "order by f.prompt ASC, f.story_score DESC, f.prompt_score DESC;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "conn = sqlite3.connect(data_base)\n",
    "df = pd.read_sql_query(sql_query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE: 242,273 for 100-700 words; 186,897 for 100 <= words <= 522; 111,644 for 100 <= words <= 348\n",
    "# Unigram: 247,115 for 100-720 words; 107,974 for 100 <= words <= 340;\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Wall time: 4min 43s for 100-348 words & 24k vocab\n",
    "# Wall time: 11min 58s for 100-710 words & 28k vocab\n",
    "df = dataset_creator.create_dataset(df=df, cpu_cores=cpu_cores, token_limit=story_token_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 110,728 for 100-348 & 185,854 for 100-522 words with BPE tokenization\n",
    "# 243,854 for 100-710 with prompt tokenization activated...\n",
    "# 246,217 for 100-720 with prompt tokenization & Unigram mode\n",
    "# 107,611 for 100-340 with prompt tokenization & Unigram mode\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration\n",
    "## BPE Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: Check story 3223 in case of bpe_707w_1024t_32k. there are whitespaces before \\n char in tokenized text\n",
    "\n",
    "idx = np.random.randint(low=0, high=len(df))\n",
    "prompt = df.prompt[idx]\n",
    "text = df.story[idx]\n",
    "tokens_prompt = encode_ids_prompt_bert(prompt)\n",
    "tokens = encode_ids_sents_bpe(text)\n",
    "print(f\"{len(tokens)} sentences\\tstory-id:{idx}\")\n",
    "print()\n",
    "print(tokens_prompt)\n",
    "print(99 * '~')\n",
    "print()\n",
    "for sent in tokens:\n",
    "    print('>', sent)\n",
    "# print(tokens)\n",
    "print(99 * '=')\n",
    "print()\n",
    "print(prompt)\n",
    "print(99 * '~')\n",
    "print()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Those two gaps are visualization errors\n",
    "sent_num = df.story_sent_num.values\n",
    "explorer.plot_hist(sent_num, range_=(3, df.story_sent_num.values.max()), hist_type='Sentence Count', value_src='Story')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_len = df.story_sent_len_max.values\n",
    "explorer.plot_hist(sent_len, range_=(7, df.story_sent_len_max.values.max()), hist_type='Max Sentence Length',\n",
    "                   value_src='Story')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_num = df.story_token_num.values\n",
    "explorer.plot_hist(token_num, range_=(111, story_token_limit + 1), hist_type='Token Count', value_src='Story')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_num = df.prompt_token_num.values\n",
    "explorer.plot_hist(token_num, range_=(5, 120), hist_type='Token Count', value_src='Prompt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_num = df.story_token_num.values.sum()\n",
    "print(f\"Sum of story tokens in corpus: {token_num:,d}\")\n",
    "# 31.9M tokens for 100-340 words with Unigram tokenization\n",
    "# 77.2M tokens for 100-522 words with BPE tokenization\n",
    "# 122M tokens for 100-720 words with Unigram tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_num = df.story_words.values.sum()\n",
    "print(f\"Sum of story words in corpus: {word_num:,d}\")\n",
    "# 24.3M words for 100-340 word stories with Unigram tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Processed Data Into Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Wall time: 1min 26s\n",
    "df.to_sql(vocab_name, conn, if_exists='replace')  # , if_exists='replace'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Created database table '{vocab_name}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
